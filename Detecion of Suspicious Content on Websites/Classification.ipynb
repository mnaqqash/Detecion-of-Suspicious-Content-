{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\t\r\n",
      "\tCollege of Electrical & Mechanical Engineering (CEME) - National University of Sciences and Technology (NUST)\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import csv\n",
    "\n",
    "def crawl(url):\n",
    "    \n",
    "    src=urllib.request.urlopen(url).read()\n",
    "    soup=bs.BeautifulSoup(src,'lxml')\n",
    "    process(soup)\n",
    "\n",
    "def process(soup):\n",
    "    \n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    ps=PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    file=open(\"text.txt\",\"w\")\n",
    "    \n",
    "\n",
    "    a=[]\n",
    "    title=soup.find('title').text\n",
    "    t=re.sub(r'^\\w]', ' ', title)\n",
    "##    file.write(t)\n",
    "##    print(t)\n",
    "    t_t=word_tokenize(t)\n",
    "##    print(t_t)\n",
    "    t1=[i for i in t_t if i.lower() not in stop_words]\n",
    "    cruft = '&`©@%?-,_./()[]{}\\^\\\\n+*^~:;!\"\"``.' + \"''...,'s<>'â{}=n't\"\n",
    "    t2=[i for i in t1 if i not in cruft]\n",
    "    a.append(t)\n",
    "    txt=soup.find('p')\n",
    "##    print(txt.text)\n",
    "    for p in soup.findAll('p'):\n",
    "        p=p.text\n",
    "        t=re.sub(r'^\\w]', ' ', p)\n",
    "##        file.append(t)\n",
    "##        print(t)\n",
    "##        p_t=word_tokenize(t)\n",
    "##        t3=[i for i in p_t if i.lower()  not in stop_words]\n",
    "##        t4=[i for i in t3 if i not in cruft]\n",
    "        a.append(t)\n",
    "##    file.close()\n",
    "##    print(a)\n",
    "    for i in a:\n",
    "        print(i)\n",
    "        file.write(i)\n",
    "    file.close()\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    url='http://www.nust.edu.pk/INSTITUTIONS/Colleges/CEME/Pages/default.aspx'\n",
    "    crawl(url)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load review data through Sklearn </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdir = r'reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data. \n",
    "review_train = load_files(reviewdir, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not Suspicious', 'suspisious']"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target names (\"classes\") are automatically generated from subfolder names\n",
    "review_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Tuesday\\xe2\\x80\\x99s deadly coordinated bomb attacks against soft targets in Iraq bear the mark of a simmering Sunni insurgency. And despite the successes of the 2007 troop surge, our unwillingness to tackle the root of the sectarian divisions in Iraq will mean further bloodshed for the people of that country.\\n\\nAs I argue in my book The Sunni-Shia Conflict: Understanding Sectarian Violence in the Middle East, one of the Bush administration\\xe2\\x80\\x99s most tragic blunders (aside from the invasion proper) was its'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First file seems to be about a reviews. \n",
    "review_train.data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reviews/suspisious/cv037_18510.txt'"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is in \"neg\" folder\n",
    "review_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is a negative review and is mapped to 0 index 'neg' in target_names\n",
    "review_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Counter Vectorizer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn off pretty printing of jupyter notebook... it generates long lines\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['Shia sunni brotherhood.',\n",
    "         'Muslim unity and peace.',\n",
    "        \"This is what Islam teaches.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CoutVectorizer to use NLTK's tokenizer instead of its \n",
    "# default one (which ignores punctuation and stopwords). \n",
    "# Minimum document frequency set to 1. \n",
    "foovec = CountVectorizer(min_df=1, tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shia': 7, 'sunni': 8, 'brotherhood': 2, '.': 0, 'muslim': 5, 'unity': 11, 'and': 1, 'peace': 6, 'this': 10, 'is': 3, 'what': 12, 'islam': 4, 'teaches': 9}"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents turned into sparse vector of word frequency counts\n",
    "sents_counts = foovec.fit_transform(sents)\n",
    "# foovec now contains vocab dictionary which maps unique words to indexes\n",
    "foovec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents_counts has a dimension of 3 (document count) by 19 (# of unique words)\n",
    "sents_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vector is small enough to view in full! \n",
    "sents_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32274454, 0.        , 0.54645401, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.54645401, 0.54645401, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.28321692, 0.47952794, 0.        , 0.        , 0.        ,\n",
       "        0.47952794, 0.47952794, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.47952794, 0.        ],\n",
       "       [0.2553736 , 0.        , 0.        , 0.43238509, 0.43238509,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.43238509,\n",
       "        0.43238509, 0.        , 0.43238509]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF values\n",
    "# raw counts have been normalized against document length, \n",
    "# terms that are found across many docs are weighted down\n",
    "sents_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Transforming  Reviews </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize movie_vector object, and then turn movie train data into a vector \n",
    "review_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)         # use all 25K words. 82.2% acc.\n",
    "# movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features = 3000) # use top 3000 words only. 78.5% acc.\n",
    "review_counts = review_vec.fit_transform(review_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'screen' is found in the corpus, mapped to index 19637\n",
    "review_vec.vocabulary_.get('screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likewise, Mr. Steven Seagal is present...\n",
    "review_vec.vocabulary_.get('seagal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 5221)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# huge dimensions! 2,000 documents, 25K unique terms. \n",
    "review_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "review_tfidf = tfidf_transformer.fit_transform(review_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 5221)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "review_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training and Testing Naive Bayes Classifier </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    review_tfidf, review_train.target, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results, find accuracy\n",
    "y_pred = clf.predict(docs_test)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5],\n",
       "       [ 0, 15]])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trying Classifier on Reviews </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very short \n",
    "with open(\"/home/naqqash/Desktop/Project ki khap/Link-Python Project/text.txt\", \"r\") as file:\n",
    "    data=file.read().replace('\\n', '')\n",
    "\n",
    "reviews_new = [data]\n",
    "reviews_new_counts = review_vec.transform(reviews_new)\n",
    "reviews_new_tfidf = tfidf_transformer.transform(reviews_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Website you are trying to access contains Not Suspicious data\n"
     ]
    }
   ],
   "source": [
    "# print out results\n",
    "for review, category in zip(reviews_new, pred):\n",
    "    print('The Website you are trying to access contains %s data' % (review_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
